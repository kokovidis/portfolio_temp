{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surprise recommender systems library with MLflow\n",
    "## Development Stage\n",
    "In this development notebook we show how we can use suprise package to train different recommender algorithms. With the use of MLflow we demonstrate how we can select our best performing algorithm in terms of accuracy (rmse) and of training/predicting time. First we import our required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from surprise import Reader, Dataset #converts pandas dataframe to surprise objects\n",
    "from surprise import NormalPredictor, KNNBasic, KNNWithMeans, KNNWithZScore, KNNBaseline #algorithms\n",
    "from surprise import SVD, BaselineOnly, SVDpp, NMF, SlopeOne, CoClustering #algorithms\n",
    "\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.accuracy import rmse\n",
    "#from surprise import accuracy\n",
    "#from surprise.model_selection import train_test_split\n",
    "\n",
    "import mlflow #load mlflow - no further setup is needed in faculty as it has a native UI.\n",
    "#Warning: if you are going to use mlflow in other environment you will need to set it up first\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our ratings files into a pandas DataFrame. We remove the timestamp column as we will not use it further. We transform pandas DataFrame to a surpise object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating\n",
       "0       1        1       5\n",
       "1       1        2       3\n",
       "2       1        3       4\n",
       "3       1        4       3\n",
       "4       1        5       3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv('/project/DataCollection/ratings.csv')\n",
    "ratings['rating'] = ratings['rating'].astype(int)\n",
    "ratings = ratings.drop('timestamp', axis=1)\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we convert it to surpise object\n",
    "reader = Reader(rating_scale=(0, 5)) #set the range of rating column\n",
    "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a new MLflow experiment instance. This will save the several metrics from the different models that we will experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'Surprise_train' does not exist. Creating a new experiment\n"
     ]
    }
   ],
   "source": [
    "# Set the experiment name to an experiment \n",
    "mlflow.end_run()\n",
    "mlflow.set_experiment(\"Surprise_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create a loop which will train different algorithms from the Surprise package. With MLflow we store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below we select several algorithms from surprise package:\n",
    "algorithms = [SVD(), SVDpp(), SlopeOne(), NormalPredictor(), \n",
    "              KNNBaseline(), KNNBasic(), KNNWithMeans(), \n",
    "              KNNWithZScore(), BaselineOnly(), CoClustering()]\n",
    "\n",
    "# Iterate over all algorithms\n",
    "for algorithm in algorithms:\n",
    "    mlflow.start_run() #for each iteration we will save the new results\n",
    "    \n",
    "    # Perform cross validation\n",
    "    results = cross_validate(algorithm, data, measures=['RMSE'], cv=3, n_jobs=-1, verbose=False)\n",
    "\n",
    "    # Get results & append algorithm name\n",
    "    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
    "    \n",
    "    #Save results to MLflow\n",
    "    mlflow.set_tag(\"Model\", tmp[3])\n",
    "    mlflow.log_param(\"TestRMSE\", str(tmp[0]))\n",
    "    mlflow.log_param(\"Training Time\", tmp[1])\n",
    "    mlflow.log_param(\"Predicting Time\", tmp[2])\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we access the Experiments sections of Faculty.ai . We access the experiment that we have logged and we examine the results:\n",
    "\n",
    "[comment]: <> (Local file is mlflow1.png)\n",
    "![](https://i.imgur.com/qLarbSP.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select all of the experiments and we request to compare them. In the scatter plot below we request to have in x'x axis the Test RMSE and in y'y axis the time for making predictions.\n",
    "After examining the different models we note down that SVD was the model that achieved both the best accuracy and predicting time:\n",
    "\n",
    "[comment]: <> (Local file is mlflow2.png)\n",
    "![](https://i.imgur.com/pXweI6M.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Stage\n",
    "Based on the best algorithm from the previous stage (SVD) we have create a **surprise_SVD_job.py** file which will be executed once a day (to update the recommendations) on Faculty.ai Jobs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This script will train model and will save in a pickle file the movie recommendations for each user.\n",
    "In the next cells we replicate the code that we can find in this script file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stage 1\n",
    "ratings = pd.read_csv('/project/DataCollection/ratings.csv')\n",
    "ratings['rating'] = ratings['rating'].astype(int)\n",
    "ratings = ratings.drop('timestamp', axis=1)\n",
    "reader = Reader(rating_scale=(0, 5)) #set the range of rating column\n",
    "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "# Select your best parameters for SVD\n",
    "param_grid = {'n_factors':[80,100,120],\n",
    "              #'n_epochs': [15,20,25], \n",
    "              #'lr_all': [0.002, 0.005, 0.01],\n",
    "              #'reg_all': [0.01,0.02,0.03],\n",
    "              'random_state':[94]\n",
    "             }\n",
    "grid_search = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3, n_jobs=-1)\n",
    "grid_search.fit(data)\n",
    "\n",
    "best_model = grid_search.best_estimator['rmse']\n",
    "\n",
    "# Get the best model\n",
    "data = data.build_full_trainset()\n",
    "best_model = best_model.fit(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the best model now we retrieve the top 10 predictions for each user. \n",
    "With the surpise's .build_anti_testset() method we create a matrix that has all of the movies that each user haven't rated so far.\n",
    "The following code has been adapted from surpise's documentation (reference in comment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stage 2\n",
    "#https://surprise.readthedocs.io/en/stable/FAQ.html?highlight=get%20top%20predictions#how-to-get-the-top-n-recommendations-for-each-user\n",
    "\n",
    "def get_top_n(predictions, n=10):\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n\n",
    "\n",
    "# Than predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "data_anti_testset = data.build_anti_testset()\n",
    "predictions = best_model.test(data_anti_testset)\n",
    "\n",
    "top_10 = get_top_n(predictions, n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(483, 5),\n",
       " (513, 4.796565258129434),\n",
       " (474, 4.771459346501919),\n",
       " (408, 4.753590453730842),\n",
       " (285, 4.743957308039365),\n",
       " (511, 4.738838663889011),\n",
       " (603, 4.72774841361666),\n",
       " (479, 4.674467842693398),\n",
       " (498, 4.620256873215769),\n",
       " (302, 4.613314636579151)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we get the prediction for the user 1.\n",
    "#For movie with ID 483, the predicted rating is 5\n",
    "top_10[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stage 3\n",
    "#Finally we save the recommendations to a pickle file in our local directory\n",
    "f = open(\"top_10_recomm.pkl\",\"wb\")\n",
    "pickle.dump(top_10,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment stage \n",
    "Below we demonstrate how we can serve these recommendations with an API through faculty.ai.\n",
    "First we have created the **svd_app.py** script which will host our Flask API. What the script does, is to load the top_10_recomm.pkl pickle file (from the previous script) and serve it as an API. The API offers a GET endpoint as the internal clients (on premises or remotely) will just request the recommendations for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = \"python3\" #requirement for faculty.ai\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"python3\" \n",
    "from flask import Flask\n",
    "from flask import request\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "flask_server = Flask(__name__)\n",
    "\n",
    "with open('top_10_recomm.pkl', 'rb') as f:  #we load the recommendations\n",
    "    top_10 = pickle.load(f)\n",
    "    \n",
    "@flask_server.route('/predict/<int:uid>', methods=['GET'])  #we define our get endpoint\n",
    "def get_top_10(uid):\n",
    "    if not top_10[uid]:\n",
    "        empty = pd.DataFrame(columns=['movieId' , 'pred_rating', 'userId'])\n",
    "        empty.loc[0,'userId']=uid\n",
    "        empty_json = empty.to_json()\n",
    "        return empty_json\n",
    "    else:\n",
    "        extr = top_10[uid]\n",
    "        extr = pd.DataFrame(extr)\n",
    "        extr['user_id'] = uid\n",
    "        extr.columns = ['movieId' , 'pred_rating', 'userId']\n",
    "        resp_json = extr.to_json()\n",
    "        return resp_json\n",
    "    \n",
    "if __name__ == '__main_':\n",
    "    logging.info('Listening on port {}'.format(8080))\n",
    "    flask_server.run(debug=True, host='0.0.0.0', port=8080, threaded=False)  #(debug=False, host='127.0.0.1', port=8080,  threaded=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy the API we access Faculty.ai > Deployments > API . We select our script file and the server object inside the script (in our case flask_server). Finally we request to generate a deployment API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[comment]: <> (Local file is facultyapi.png)\n",
    "![](https://i.imgur.com/j4Il1O7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In the deploy tab "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
