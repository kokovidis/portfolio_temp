{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dockerize an API and deploy it on other cloud VMs\n",
    "In this chapter we created an application which we finally hosted on Docker Hub.\n",
    "The app is actually a POST API take new -unseen- user ratings through a json file, return recommendations and updates its existing rating file with the new ratings. The script is written in Apache Spark (pyspark package).\n",
    "Below you will find short descriptions of the the script files and how they are used in the app.\n",
    "\n",
    "## initialize_data.py \n",
    "This file takes the original dataset from our directory and creates a new one without the test users (these that we are going to use in our client side)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "#!pip install pyspark\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark import SQLContext\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import IntegerType, FloatType, StringType\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = \"python3\"  #uncomment for Faculty.ai (dev) / comment for docker (deployment)\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"python3\" #uncomment for Faculty.ai (dev) / comment for docker (deployment)\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import time ###calculate execution time\n",
    "\n",
    "# Get a spark context\n",
    "\n",
    "conf = SparkConf().set(\"spark.master\", \"local[*]\").set(\"spark.sql.crossJoin.enabled\",\"true\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RATINGS DATASET \n",
    "###################\n",
    "small_ratings_DF = sqlContext.read.csv(\"/project/DataCollection/ratings.csv\", inferSchema=True, header = True)\n",
    "    #remove query users! (those who are requested to retrieve recommendations from the assignment)\n",
    "small_ratings_DF = small_ratings_DF.filter(~small_ratings_DF.userId.isin([198,11,314,184,163,710,881,504,267,653]))\n",
    "    #remove timestaps\n",
    "small_ratings_DF = small_ratings_DF.drop('timestamp') \n",
    "\n",
    "#preprocessing \n",
    "small_ratings_DF = small_ratings_DF.withColumn(\"userId\", small_ratings_DF[\"userId\"].cast(IntegerType()))\n",
    "small_ratings_DF = small_ratings_DF.withColumn(\"movieId\", small_ratings_DF[\"movieId\"].cast(IntegerType()))\n",
    "small_ratings_DF = small_ratings_DF.withColumn(\"rating\", small_ratings_DF[\"rating\"].cast(FloatType()))\n",
    "\n",
    "###UPDATE THE RATINGS FILE\n",
    "small_ratings_DF.repartition(1).write.csv(path=\"/project/1. ServersSetup/1.2 DockerAPI/Generated Data Files/ratings_upd.csv\", mode=\"append\")\n",
    "\n",
    "\n",
    "### MOVIE DATASET (we just extract the file to our directory)\n",
    "####################\n",
    "movies_DF = sqlContext.read.csv(\"/project/DataCollection/movies.csv\", inferSchema=True, header = True)\n",
    "movies_DF.repartition(1).write.csv(path=\"/project/1. ServersSetup/1.2 DockerAPI/Generated Data Files/movies_upd.csv\", mode=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross_validation.py\n",
    "This script file takes the existing ratings and performs a grid search for the ALS algorithm.\n",
    "It saves the best parameters in a dictionary and the execution time of the script in a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "### RATINGS DATASET\n",
    "##################\n",
    "small_ratings_DF = sqlContext.read.csv(\"/project/1. ServersSetup/1.2 DockerAPI/Generated Data Files/ratings_upd.csv\", inferSchema=True, header = True)\n",
    "\n",
    "#### TRAIN DATASET\n",
    "##################\n",
    "    ##Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=3, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "         coldStartStrategy=\"drop\") #coldStartStrategy=\"drop\" to get valid results\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(als.regParam, [0.03,0.1,0.3]).addGrid(als.rank, [3,10,30]).build()\n",
    "\n",
    "regEval = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\") #predictionCol=\"prediction\"\n",
    "crossVal = CrossValidator(estimator=als, estimatorParamMaps=paramGrid, evaluator=regEval, seed=94)\n",
    "cvModel = crossVal.fit(small_ratings_DF)\n",
    "\n",
    "#### EXTRACT BEST PARAMETERS FROM GRID SEARCH CV - SAVE THEM TO JSON\n",
    "######################################################\n",
    "best_dict = cvModel.getEstimatorParamMaps()[np.argmin(cvModel.avgMetrics)] #argmin get the lowest rmse \n",
    "best_dict_list = list(best_dict.values())\n",
    "best_dict_list  ### [0]:regParam , [1]:rank\n",
    "best_dict_res = {'regParam':best_dict_list[0], 'rank': best_dict_list[1] }\n",
    "\n",
    "with open('/project/1. ServersSetup/1.2 DockerAPI/Generated Data Files/best_params.json', 'w') as f:\n",
    "    json.dump(best_dict_res, f)\n",
    "\n",
    "ex_time = str(\"--- Execution time : %s seconds ---\" % (time.time() - start_time))\n",
    "print(ex_time)\n",
    "\n",
    "with open(\"/project/1. ServersSetup/1.2 DockerAPI/Generated Data Files/ex_time.txt\", \"w\") as text_file:\n",
    "    text_file.write(ex_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# app.py\n",
    "This file will actually host our API. Before the actual servers we load the movies and rating file. In addition we create a DataFrame which hosts the total number of ratings (out of all of existing users) for each movie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###MOVIES DATASET\n",
    "##################\n",
    "small_movies =sqlContext.read.csv(\"/project/1. ServersSetup/1.2 DockerAPI/Generated Data Files/movies_upd.csv\", inferSchema=True, header = True)\n",
    "small_movies_data_DF = small_movies.withColumn(\"movieId\", small_movies[\"movieId\"].cast(IntegerType()))\n",
    "#small_movies_data_DF.show(5)\n",
    "\n",
    "### RATINGS DATASET\n",
    "##################\n",
    "    #load    \n",
    "small_ratings_DF = sqlContext.read.csv(\"/project/1. ServersSetup/1.2 DockerAPI/Generated Data Files/ratings_upd.csv\", inferSchema=True, header = True)\n",
    "       #remove timestaps\n",
    "small_ratings_DF = small_ratings_DF.drop('timestamp')    \n",
    "\n",
    "#Get the total ratings for each movie \n",
    "####################################\n",
    "#will be used to return prediction with high number (count) of existing reviews\n",
    "rati_count = small_ratings_DF.groupBy('movieId').count()\n",
    "rati_count = rati_count.withColumnRenamed(\"count\", \"TotalReviews\")\n",
    "#rati_count.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create a synthetic query example. The obj will be actually our json request to our API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Synthetic Query Example\n",
    "df = pd.read_csv('/project/DataCollection/ratings.csv')\n",
    "sub = df.loc[df.userId==11].drop('timestamp', axis=1)\n",
    "#sub = sub[0:2]\n",
    "obj = json.loads(sub.to_json())   \n",
    "print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following subscript, part of the app.py is part of the Flask server.\n",
    "In the beginning we use the pandas library to load the json file, as it was the most convienient way to load a json file to pyspark DataFrame.In the next step we create an empty model with the best parameters frm the cross-validation.py script. We merge the new ratings with out existing ratings files and we train our model.\n",
    "Finally, we create a matrix with all of the movies IDs that the new user has not seen. For these movies we predict their ratings. From these ratings we keep the movies with the highest ratings and with at least 30 views from other users. The client will receive the top 20 recommendations in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame.from_dict({ \"userId\": { \"1259\": 11, \"1260\": 11, \"1261\": 11, \"1262\": 11, \"1263\": 11, \"1264\": 11, \"1265\": 11, \"1266\": 11, \"1267\": 11, \"1268\": 11, \"1269\": 11, \"1270\": 11, \"1271\": 11, \"1272\": 11, \"1273\": 11, \"1274\": 11, \"1275\": 11, \"1276\": 11, \"1277\": 11, \"1278\": 11, \"1279\": 11, \"1280\": 11, \"1281\": 11, \"1282\": 11, \"1283\": 11, \"1284\": 11, \"1285\": 11, \"1286\": 11, \"1287\": 11, \"1288\": 11, \"1289\": 11, \"1290\": 11, \"1291\": 11, \"1292\": 11, \"1293\": 11, \"1294\": 11, \"1295\": 11, \"1296\": 11, \"1297\": 11, \"1298\": 11, \"1299\": 11, \"1300\": 11, \"1301\": 11, \"1302\": 11, \"1303\": 11, \"1304\": 11, \"1305\": 11, \"1306\": 11, \"1307\": 11, \"1308\": 11, \"1309\": 11, \"1310\": 11, \"1311\": 11, \"1312\": 11, \"1313\": 11, \"1314\": 11, \"1315\": 11, \"1316\": 11, \"1317\": 11, \"1318\": 11, \"1319\": 11, \"1320\": 11, \"1321\": 11, \"1322\": 11 }, \"movieId\": { \"1259\": 6, \"1260\": 10, \"1261\": 36, \"1262\": 44, \"1263\": 95, \"1264\": 110, \"1265\": 150, \"1266\": 153, \"1267\": 165, \"1268\": 170, \"1269\": 208, \"1270\": 292, \"1271\": 318, \"1272\": 349, \"1273\": 356, \"1274\": 368, \"1275\": 376, \"1276\": 377, \"1277\": 380, \"1278\": 434, \"1279\": 457, \"1280\": 466, \"1281\": 474, \"1282\": 480, \"1283\": 493, \"1284\": 511, \"1285\": 529, \"1286\": 589, \"1287\": 593, \"1288\": 648, \"1289\": 733, \"1290\": 736, \"1291\": 780, \"1292\": 1100, \"1293\": 1101, \"1294\": 1210, \"1295\": 1358, \"1296\": 1370, \"1297\": 1385, \"1298\": 1391, \"1299\": 1408, \"1300\": 1438, \"1301\": 1518, \"1302\": 1552, \"1303\": 1573, \"1304\": 1584, \"1305\": 1586, \"1306\": 1597, \"1307\": 1604, \"1308\": 1608, \"1309\": 1616, \"1310\": 1687, \"1311\": 1693, \"1312\": 1704, \"1313\": 1721, \"1314\": 1784, \"1315\": 1840, \"1316\": 1882, \"1317\": 1917, \"1318\": 1918, \"1319\": 1923, \"1320\": 2002, \"1321\": 2027, \"1322\": 2028 }, \"rating\": { \"1259\": 5, \"1260\": 3, \"1261\": 4, \"1262\": 2, \"1263\": 3, \"1264\": 5, \"1265\": 5, \"1266\": 3, \"1267\": 3, \"1268\": 4, \"1269\": 3, \"1270\": 4, \"1271\": 4, \"1272\": 5, \"1273\": 5, \"1274\": 3, \"1275\": 2, \"1276\": 3, \"1277\": 4, \"1278\": 3, \"1279\": 5, \"1280\": 3, \"1281\": 4, \"1282\": 4, \"1283\": 3, \"1284\": 4, \"1285\": 5, \"1286\": 4, \"1287\": 5, \"1288\": 4, \"1289\": 4, \"1290\": 4, \"1291\": 4, \"1292\": 3, \"1293\": 5, \"1294\": 4, \"1295\": 4, \"1296\": 3, \"1297\": 3, \"1298\": 1, \"1299\": 5, \"1300\": 3, \"1301\": 4, \"1302\": 4, \"1303\": 3, \"1304\": 5, \"1305\": 4, \"1306\": 4, \"1307\": 4, \"1308\": 4, \"1309\": 3, \"1310\": 3, \"1311\": 5, \"1312\": 4, \"1313\": 5, \"1314\": 5, \"1315\": 4, \"1316\": 2, \"1317\": 4, \"1318\": 4, \"1319\": 4, \"1320\": 2, \"1321\": 3, \"1322\": 5 } })\n",
    "df = pd.DataFrame.from_dict(obj)\n",
    "sp_df = sqlContext.createDataFrame(df)\n",
    "\n",
    "### Load best hyper-parameters\n",
    "with open('/project/1. ServersSetup/1.2 DockerAPI/Generated Data Files/best_params.json') as f:\n",
    "    import_param = json.load(f)\n",
    "\n",
    "### CREATE AN EMPTY MODEL WITH THE BEST hyPARAMS\n",
    "\n",
    "als = ALS(maxIter=3, regParam=import_param['regParam'], rank=import_param['rank'], userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "     coldStartStrategy=\"drop\")\n",
    "\n",
    "###GET PARSED USER ID\n",
    "new_user_ID = sp_df.first().userId\n",
    "### UPDATE THE EXISTING TABLE WITH THE NEW RATINGS\n",
    "small_ratings_DF_upd = small_ratings_DF.union(sp_df)\n",
    "\n",
    "### CREATE A LIST WITH MOVIE_IDS THAT THE USER HAS RATED\n",
    "sp_df_rated = sp_df.select('movieId').rdd.map(list).map(lambda x: x[0])\n",
    "sp_df_rated_list = sp_df_rated.collect()\n",
    "\n",
    "### CREATE A DF WITH THE MOVIES THAT THE USER HAS NOT RATED (ALL MOVIES - RATED MOVIES)\n",
    "new_user_unrated_movies_DF = small_movies_data_DF.filter(~small_movies_data_DF.movieId.isin(sp_df_rated_list))\n",
    "#.map(lambda x: (new_user_ID, x[0]))\n",
    "#.map(lambda x: (198, x[0])))\n",
    "\n",
    "###Sanity Checks\n",
    "#small_movies_data_DF.count()\n",
    "\n",
    "    ###Preprocessing\n",
    "new_user_unrated_movies_DF = new_user_unrated_movies_DF.drop('title')\n",
    "new_user_unrated_movies_DF = new_user_unrated_movies_DF.withColumn('userId', lit(new_user_ID))\n",
    "new_user_unrated_movies_DF = new_user_unrated_movies_DF.select('userId', 'movieId') #re-arrange columns \n",
    "\n",
    "#### TRAIN THE MODEL WITH ALL THE PREVIOUS RATINGS + NEW RECEIVED RATINGS\n",
    "model = als.fit(small_ratings_DF_upd)\n",
    "\n",
    "### PREDICT\n",
    "#use the model to predict ratings for the rest movies of the user\n",
    "new_user_recommendations_DF = model.transform(new_user_unrated_movies_DF)\n",
    "\n",
    "#get the total number of pre-existing reviews for each movie\n",
    "new_user_recommendations_DF = new_user_recommendations_DF.join(rati_count, new_user_recommendations_DF.movieId == rati_count.movieId).drop(rati_count.movieId)\n",
    "\n",
    "#order by the highest rated predictions\n",
    "new_user_recommendations_DF = new_user_recommendations_DF.orderBy(new_user_recommendations_DF.prediction.desc())\n",
    "\n",
    "#filter out movies with less than 30 reviews\n",
    "new_user_recommendations_DF = new_user_recommendations_DF.filter(new_user_recommendations_DF.TotalReviews>30) #(returns around 10%)\n",
    "\n",
    "resp = new_user_recommendations_DF.na.drop(subset=[\"prediction\"])\n",
    "resp = resp.limit(20)\n",
    "\n",
    "#get movie title\n",
    "resp = resp.join(small_movies_data_DF, resp.movieId == small_movies_data_DF.movieId).drop(small_movies_data_DF.movieId)\n",
    "resp = resp.select('userId', 'title', 'prediction')\n",
    "resp = resp.orderBy(resp.prediction.desc())\n",
    "\n",
    "#### CONVERT PYSPARK DF--> PANDAS DF --> DICT --> JSON RESPONSE\n",
    "resp_pd = resp.toPandas()\n",
    "resp_json = resp_pd.to_json()\n",
    "\n",
    "### Write the new ratings to the parquet file (update existing ones)\n",
    "small_ratings_DF_upd.repartition(1).write.csv(path=\"/project/DataCollection/ratings_upd.csv\", mode=\"append\")\n",
    "#small_ratings_DF_upd.write.parquet(\"ratings_upd.parquet\", mode='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we display an example of the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = json.loads(resp_json)   \n",
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full directory has been uploaded to a GCP VM instance, has been dockerized and uploaded to docker hub. Then from an AWS EC2 machine, we pulled the app from the repo and we ran it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
